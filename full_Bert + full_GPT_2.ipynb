{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data: \n",
    "\n",
    "os.chdir('/Users/brandoncheong/Desktop/ML_Dissertation/wiki-pages')\n",
    "\n",
    "train_data = np.load('train_data.npy', allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = [json.loads(string.replace('\"\"','\\\"\"')) for string in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 75397, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'evidence': [[[92206, 104971, 'Nikolaj_Coster-Waldau', 7], [92206, 104971, 'Fox_Broadcasting_Company', 0]]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145449\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 113942, 'verifiable': 'NOT VERIFIABLE', 'label': 'NOT ENOUGH INFO', 'claim': 'Bob Dylan has been inducted into four different colleges.', 'evidence': [[[133857, None, None, None]]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_data[102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data_new = list()\n",
    "\n",
    "for dictionary in train_data:\n",
    " \n",
    "    for element in dictionary['evidence']:\n",
    "        \n",
    "        if len(element) == 0 and dictionary['evidence'].index(element) != len(dictionary['evidence']) - 1:\n",
    "\n",
    "            continue\n",
    "            \n",
    "        elif len(element) == 0 and dictionary['evidence'].index(element) == len(dictionary['evidence']) - 1:\n",
    "            case = False\n",
    "\n",
    "            break\n",
    "            \n",
    "        elif dictionary['evidence'].index(element) != len(dictionary['evidence']) - 1 and type(element[0]) != str:\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        \n",
    "        elif dictionary['evidence'].index(element) == len(dictionary['evidence']) - 1 and type(element[0]) != str:\n",
    "            case = False\n",
    "\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            case = True\n",
    "\n",
    "            idx = dictionary['evidence'].index(element) \n",
    "            \n",
    "    \n",
    "    if case == True and idx == len(dictionary['evidence']) - 1:\n",
    "        \n",
    "            \n",
    "        text = dictionary['evidence'][idx]        \n",
    "        \n",
    "        alt_dictionary = copy.deepcopy(dictionary)\n",
    "        alt_dictionary['evidence'] = text[0]\n",
    "        full_train_data_new.append(alt_dictionary)\n",
    "    \n",
    "    \n",
    "    elif case == True and idx < len(dictionary['evidence']) - 1:  \n",
    "        \n",
    "        texts = dictionary['evidence'][idx:] # adapt to specific structure of the data: this is a list of lists\n",
    "        initial_text = texts[0]\n",
    "        for text in texts[1:]:\n",
    "            initial_text += text\n",
    "        \n",
    "        alt_dictionary = copy.deepcopy(dictionary)\n",
    "\n",
    "        alt_dictionary['evidence'] = initial_text\n",
    "        full_train_data_new.append(alt_dictionary)\n",
    "    \n",
    "    else: # idx does not exist\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d59c1a8c9f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_train_data_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(full_train_data_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEI time:\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "\n",
    "\n",
    "# obtain full dataset consisting of original claims + Bert-generated evidence_statements:\n",
    "\n",
    "bert_nei_data = list()\n",
    "for dictionary in full_train_data_new:\n",
    "    if dictionary['label'] == 'NOT ENOUGH INFO':\n",
    "           entry = copy.deepcopy(dictionary)\n",
    "           NER_on_claim = NER(entry['claim'])\n",
    "    # set all claims to 'empty' if not appropriate - will skip this in training data\n",
    "           if len(list(NER_on_claim.ents)) == 0:\n",
    "                entry['claim'] = 'Empty'\n",
    "                continue\n",
    "           else: \n",
    "                masked_entity = list(NER_on_claim.ents)[-1].text\n",
    "                masked_entity_start_idx = entry['claim'].index(masked_entity)\n",
    "                masked_entity_end_idx = masked_entity_start_idx + len(masked_entity) + 1\n",
    "                text = entry['claim'][:masked_entity_start_idx] + '[MASK]' + entry['claim'][masked_entity_end_idx:]\n",
    "                if text[-1] == '.':\n",
    "                       pass\n",
    "                else:\n",
    "                       text += '.'\n",
    "                print(text)\n",
    "                prediction = unmasker(text)[0]['token_str']\n",
    "                print(prediction)\n",
    "                entry['evidence'] = entry['claim'][:masked_entity_start_idx] + prediction + entry['claim'][masked_entity_end_idx:]\n",
    "                bert_nei_data.append(entry) \n",
    "                print(f'Entry {baseline_20_train_data.index(entry)} done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, time to work on GPT-2: \n",
    "\n",
    "masked_train_data = copy.deepcopy(full_train_data_new)\n",
    "\n",
    "for entry in masked_train_data:\n",
    "    NER_on_claim = NER(entry['claim'])\n",
    "    if len(list(NER_on_claim.ents)) == 0:\n",
    "        entry['claim'] = 'Empty'\n",
    "        continue\n",
    "    else: \n",
    "        masked_entity = list(NER_on_claim.ents)[-1].text\n",
    "    masked_entity_idx = entry['claim'].index(masked_entity)\n",
    "    entry['claim'] = entry['claim'][:masked_entity_idx]\n",
    "    print(f'Entry {masked_train_data.index(entry)} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GPT2 on Fever by generating evidence_statements (prompts):\n",
    "\n",
    "# Note: max_length, num_return_seq both hyperparameters - can fine-tune!\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "\n",
    "full_GPT2_train_data = copy.deepcopy(masked_train_data)\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "for entry in full_GPT2_train_data:\n",
    "    prompt = entry['claim']\n",
    "    ev_statement = generator(prompt, max_length=50, num_return_sequences=1)[0]\n",
    "    entry['evidence'] = ev_statement\n",
    "    print(f'Done with {full_GPT2_train_data.index(entry)}th entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final step\n",
    "for i in range(len(full_GPT2_train_data)):\n",
    "    \n",
    "    full_GPT2_train_data[i]['claim'] = full_train_data_new[i]['claim']\n",
    "    full_GPT2_train_data[i]['evidence'] = full_GPT2_train_data[i]['evidence']['generated_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
